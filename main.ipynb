{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio(\"data/balanced\", output=\"data/splits\", seed=2022, ratio=(.7, .1, .2), group_prefix=None, move=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change plain strings in files into csv files with headers text and class using pandas\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import glob\n",
    "# path=\"processed/val/pos\"\n",
    "# def create_csv(path):\n",
    "#     files = glob.glob(path)\n",
    "#     df = pd.DataFrame(columns=['text', 'class'])\n",
    "#     for file in files:\n",
    "#         with open(file, 'r') as f:\n",
    "#             text = f.read()\n",
    "#             df = df.append({'text': text, 'class': \"+\"}, ignore_index=True)\n",
    "#     return df\n",
    "\n",
    "# df = create_csv(path + \"/pos_labled.txt\")\n",
    "# df.to_csv(path , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize and remove stop words and remove punctuation and lower case all letters in strings in files in certain path using nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "path=\"data/processed/train/pos\"\n",
    "def lemmatize(path):\n",
    "    files = glob.glob(path)\n",
    "    df = pd.DataFrame(columns=['text', 'class'])\n",
    "    for file in files:\n",
    "        with open(file,encoding=\"utf8\") as f:\n",
    "            text = f.read()\n",
    "            print(text)\n",
    "            text = text.lower()\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            text = text.split()\n",
    "            text = [lemmatizer.lemmatize(word) for word in text if not word in stop_words]\n",
    "            text = ' '.join(text)\n",
    "            df = df.append({'text': text, 'class': \"+\"}, ignore_index=True)\n",
    "            print(\"-------------------\")\n",
    "            print(text)\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "    return df\n",
    "\n",
    "df = lemmatize(path + \"/*.txt\")\n",
    "df.to_csv(path + \"/pos_labled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join two csv files randomly into one csv file using pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.read_csv(\"data/processed/test/test-neg.csv\")\n",
    "df2 = pd.read_csv(\"data/processed/test/test-pos.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.to_csv(\"data/processed/test/test.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging data split files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "path=\"data/splits/val/neg\"\n",
    "def collect(path):\n",
    "    files = glob.glob(path)\n",
    "    df = pd.DataFrame(columns=['text', 'class'])\n",
    "    for file in files:\n",
    "        with open(file,encoding=\"utf8\") as f:\n",
    "            text = f.read()\n",
    "            df = df.append({'text': text, 'class': \"-\"}, ignore_index=True)\n",
    "    return df\n",
    "df = collect(path + \"/*.txt\")\n",
    "df.to_csv(path + \"/val-neg.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cobining nonprocessed csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.read_csv(\"data/splits/val/val-neg.csv\")\n",
    "df2 = pd.read_csv(\"data/splits/val/val-pos.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.to_csv(\"data/splits/val/val.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "475a401d5f8531da0d5ec3d32485cfac990b88381cdc99d4b948e204c9710bec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
