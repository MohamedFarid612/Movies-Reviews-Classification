{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXI1M2BD_Zyl"
      },
      "source": [
        "# Movie Review Classification Using State-of-the-art NLP Model BERT "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YMRjLIbWAt1f",
        "outputId": "97065b5e-47be-42b5-e6c1-9578ed1f0c9b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSwM0O1rk0SB",
        "outputId": "ba799af1-8c61-4a22-c65d-270cb3eaa3f1"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlgXuS0l_h6m"
      },
      "source": [
        "## First: Installing and importing our pre-requisite libraries\n",
        "\n",
        "1. __transformers__: This library is used to utilize the state-of-the-art NLP model BERT\n",
        "2. __pandas__: This utility library is used to manipulate data efficiently and effectively\n",
        "3. __tqdm__: This library is used to track the progress of any ongoing iterable process\n",
        "4. __nltk__: This library is used to pre-process data before giving them to the NLP model\n",
        "5. __torch__: Pytorch library, used to create and manage neural networks\n",
        "\n",
        "To install pytorch on a CUDA-capable machine, run this command in the terminal:\n",
        "```bash\n",
        "pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9M2wkw-9e7f",
        "outputId": "ff2b9896-86cf-46da-c9ff-82040e1c1417"
      },
      "outputs": [],
      "source": [
        "# LAW runneit da wedak error khaliha !pip badal %pip\n",
        "\n",
        "%pip install transformers\n",
        "%pip install pandas\n",
        "%pip install tqdm\n",
        "%pip install nltk\n",
        "%pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHiYlT49puKX"
      },
      "outputs": [],
      "source": [
        "# Run yabny w law edak error run el fo2\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "# import splitfolders\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import os\n",
        "import glob\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muviY-I2puKZ"
      },
      "source": [
        "## Text Pre-Processing\n",
        "\n",
        "First, we pre-process our dataset to prepare it for training our model effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zy7uidInLxF"
      },
      "source": [
        "### Splitting the folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x--LITnpuKa"
      },
      "outputs": [],
      "source": [
        "splitfolders.ratio(\"data/balanced\", output=\"data/splits\", seed=2022, ratio=(.7, .1, .2), group_prefix=None, move=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woHZkCREFZ5z"
      },
      "source": [
        "### Cleaning strings in text files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjzkwu56Fe0F"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "path=\"data/processed/train/pos\"\n",
        "def lemmatize(path):\n",
        "    files = glob.glob(path)\n",
        "    df = pd.DataFrame(columns=['text', 'class'])\n",
        "    for file in files:\n",
        "        with open(file,encoding=\"utf8\") as f:\n",
        "            text = f.read()\n",
        "            text = text.lower()\n",
        "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "            text = text.split()\n",
        "            text = [lemmatizer.lemmatize(word) for word in text if not word in stop_words]\n",
        "            text = ' '.join(text)\n",
        "            df = df.append({'text': text, 'class': \"+\"}, ignore_index=True)\n",
        "    return df\n",
        "\n",
        "df = lemmatize(path + \"/*.txt\")\n",
        "df.to_csv(path + \"/pos_labled.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VllYN-u3Fj88"
      },
      "source": [
        "### combining processed csvs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yBSSFYiFoMZ"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"data/processed/test/test-neg.csv\")\n",
        "df2 = pd.read_csv(\"data/processed/test/test-pos.csv\")\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df.to_csv(\"data/processed/test/test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7S3LNlDFrTM"
      },
      "source": [
        "### merging data split files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQX43B0YFuDs"
      },
      "outputs": [],
      "source": [
        "\n",
        "path=\"data/splits/val/neg\"\n",
        "def collect(path):\n",
        "    files = glob.glob(path)\n",
        "    df = pd.DataFrame(columns=['text', 'class'])\n",
        "    for file in files:\n",
        "        with open(file,encoding=\"utf8\") as f:\n",
        "            text = f.read()\n",
        "            df = df.append({'text': text, 'class': \"-\"}, ignore_index=True)\n",
        "    return df\n",
        "df = collect(path + \"/*.txt\")\n",
        "df.to_csv(path + \"/val-neg.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8VabhXLFxYv"
      },
      "source": [
        "### combining unprocessed csvs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1qTc8CMF0B_"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"data/splits/val/val-neg.csv\")\n",
        "df2 = pd.read_csv(\"data/splits/val/val-pos.csv\")\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df.to_csv(\"data/splits/val/val.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjmHzlbLR-S9"
      },
      "source": [
        "## Reading from the CSVs\n",
        "\n",
        "Now we need to read our processed and unprocessed CSVs into corresponding dataframes using pandas.\n",
        "\n",
        "The reason we read in unprocessed data as well is, we are going to train once with processed and another with unprocessed data and check to see the difference in model competence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3ASqiU7SCBQ"
      },
      "outputs": [],
      "source": [
        "# Run yabny bas zabbat el path\n",
        "\n",
        "# Read-in the processed data\n",
        "df_train_proc = pd.read_csv(\"drive/MyDrive/data/processed/train/train.csv\")\n",
        "df_val_proc = pd.read_csv(\"drive/MyDrive/data/processed/val/val.csv\")\n",
        "df_test_proc = pd.read_csv(\"drive/MyDrive/data/processed/test/test.csv\")\n",
        "\n",
        "# Read-in the unprocessed data\n",
        "df_train_raw = pd.read_csv(\"drive/MyDrive/data/splits/train/s-train.csv\")\n",
        "df_val_raw = pd.read_csv(\"drive/MyDrive/data/splits/val/s-val.csv\")\n",
        "df_test_raw = pd.read_csv(\"drive/MyDrive/data/splits/test/s-test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFqTIrrh_wme"
      },
      "source": [
        "## Creating classes we're going to work with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJEzy_nqpuKj"
      },
      "source": [
        "### Identify global variables we're going to work with\n",
        "\n",
        "- _tokenizer_: BertTokenizer object used to convert text into tokens for BERT to work with\n",
        "- _label_ids_: Dictionary for each label with a corresponding numeric ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CFnnoZXpuKk"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained( 'bert-base-cased' )\n",
        "label_ids = {'-' : 0, '+' : 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diu0G60tpuKk"
      },
      "source": [
        "### First: Dataset class to hold our data\n",
        "\n",
        "This class will contain lists of processed model-ready data. Note that processing here is different from text pre-processing we've done earlier. This one relates to tokenization and label IDing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCY9tRc3-cW3"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.labels = [label_ids[verdict] for verdict in df['class']]\n",
        "        self.texts = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, \n",
        "                                    return_tensors=\"pt\") for text in tqdm( iterable=df['text'], desc='Tokenizing' )]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    # Fetch a batch of labels\n",
        "    def get_batch_labels(self, idx):\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    # Fetch a batch of inputs\n",
        "    def get_batch_texts(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs5uuROLA382"
      },
      "source": [
        "### Second: NeuralNetwork class to hold our model\n",
        "\n",
        "#### <u>Description of our neural network</u>:\n",
        "- The network's input layer takes in 768 inputs corresponding to the vector provided by BERT's pooled output (classification output)\n",
        "- Our network consists of 4 hidden layers with 512, 256, 128, 64 units respectively.\n",
        "- The output layer gives sigmoid function to classify the input vector\n",
        "\n",
        "#### <u>Structure of every layer</u>:\n",
        "Every layer in our neural network consists of three sublayers. Here they are in order:\n",
        "1. __Dropout Layer__: This layer zeros out some parameters passing through it by a given probability. This is useful for avoiding overfitting. Said probability is given through the 'dropout_probability' argument in the constructor.\n",
        "2. __Linear Layer__: This layer contains all of our main computation neurons which give us linear functions for fitting the model.\n",
        "3. __Activation Layer__: This layer is responsible for applying activation function and converting the linear outputs into non-linear, cascadable outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE4nkSZkA9Ky"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout_probability=0.4, use_dropout=True):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.use_dropout = use_dropout\n",
        "        self.network = []\n",
        "\n",
        "        # Input Layer to Hidden layer 1\n",
        "        self.dropout1 = nn.Dropout( 0.0 )\n",
        "        self.linear1 = nn.Linear( in_features=768, out_features=512 )\n",
        "        self.relu1 = nn.ReLU( self.linear1 )\n",
        "        self.network.append( [self.dropout1, self.linear1, self.relu1] ) \n",
        "\n",
        "        # Hidden layer 1 to Hidden layer 2\n",
        "        self.dropout2 = nn.Dropout( dropout_probability )\n",
        "        self.linear2 = nn.Linear( in_features=512, out_features=256 )\n",
        "        self.relu2 = nn.ReLU( self.linear2 )\n",
        "        self.network.append( [self.dropout2, self.linear2, self.relu2] )\n",
        "        \n",
        "        # Hidden layer 2 to Hidden layer 3\n",
        "        self.dropout3 = nn.Dropout( dropout_probability )\n",
        "        self.linear3 = nn.Linear( in_features=256, out_features=128 )\n",
        "        self.relu3 = nn.ReLU( self.linear3 )\n",
        "        self.network.append( [self.dropout3, self.linear3, self.relu3] )\n",
        "\n",
        "        # Hidden layer 3 to Hidden layer 4\n",
        "        self.dropout4 = nn.Dropout( dropout_probability )\n",
        "        self.linear4 = nn.Linear( in_features=128, out_features=64 )\n",
        "        self.relu4 = nn.ReLU( self.linear4 )\n",
        "        self.network.append( [self.dropout4, self.linear4, self.relu4] )\n",
        "\n",
        "        # Hidden Layer 4 to Output\n",
        "        self.dropout5 = nn.Dropout( dropout_probability )\n",
        "        self.linear5 = nn.Linear( in_features=64, out_features=1 )\n",
        "        self.output = nn.Sigmoid()\n",
        "        self.network.append( [self.dropout5, self.linear5, self.output] )\n",
        "\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        DROPOUT_LAYER, LINEAR_LAYER, ACTIVATION_LAYER = 0, 1, 2\n",
        "\n",
        "        _, pooled_output = self.bert( input_ids= input_id, attention_mask=mask, return_dict=False )\n",
        "\n",
        "        layer_input = pooled_output\n",
        "        for layer in self.network:\n",
        "            # Dropout\n",
        "            dropout_output = layer[ DROPOUT_LAYER ]( layer_input ) \\\n",
        "                             if self.use_dropout else layer_input\n",
        "            \n",
        "            # Linear Computation\n",
        "            linear_output = layer[ LINEAR_LAYER ]( dropout_output )\n",
        "\n",
        "            # Activation\n",
        "            layer_input = layer[ ACTIVATION_LAYER ]( linear_output )\n",
        "        \n",
        "        final_layer = layer_input\n",
        "\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88uQIS9n_7Ek"
      },
      "source": [
        "## Model Training, Validation & Testing\n",
        "\n",
        "Now, we:\n",
        "- Use the validation dataset to tune hyperparameters twice (for model using raw data and model using pre-processed data)\n",
        "- Write the function that will train our model using the training dataset\n",
        "- Write the function that will evaluate our model using the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Do9uJdpuKm"
      },
      "source": [
        "### Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJiS-_JJpuKm"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "# This model uses pre-processed data\n",
        "neural_network_model_proc = NeuralNetwork(use_dropout=False)\n",
        "\n",
        "# This model uses raw data\n",
        "neural_network_model_raw = NeuralNetwork(use_dropout=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcD9wAgwpuKn"
      },
      "source": [
        "### Some functions to improve readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGg-KFatmdud"
      },
      "outputs": [],
      "source": [
        "# Run yabny bas zabbat el path\n",
        "\n",
        "SAVE_LOAD_DESTINATION = 'drive/MyDrive/ai/assignment4/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pzW2bv7puKn"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "def propagate_forwards( model, input, label, device=\"cpu\" ):\n",
        "    label = label.to(device)\n",
        "    mask = input['attention_mask'].to(device)\n",
        "    input_id = input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "    model_output = model(input_id, mask)\n",
        "\n",
        "    return model_output.squeeze(1), label.float()\n",
        "\n",
        "def apply_cuda( model, loss_fn ):\n",
        "    return model.cuda(), loss_fn.cuda()\n",
        "\n",
        "def save_checkpoint( model, optimizer, epoch_num, learning_rate, epoch_scores, model_name ):\n",
        "\n",
        "    checkpoint_path =   SAVE_LOAD_DESTINATION + \\\n",
        "                        model_name + ' lr_' + str(learning_rate) + '_' + \\\n",
        "                        ' epoch_' + str(epoch_num) + '_' + '.pt'\n",
        "\n",
        "    model_checkpoint = {'model_state':model.state_dict(), \n",
        "                    'optim_state':optimizer.state_dict(), \n",
        "                    'n_epoch':epoch_num, 'lr':learning_rate,\n",
        "                    'epoch_scores':epoch_scores}\n",
        "\n",
        "    torch.save( model_checkpoint, checkpoint_path )\n",
        "\n",
        "def load_checkpoint( model, optimizer, epoch_num, learning_rate, model_name ):\n",
        "\n",
        "    checkpoint_path = SAVE_LOAD_DESTINATION + \\\n",
        "                        model_name + ' lr_' + str(learning_rate) + '_' + \\\n",
        "                        ' epoch_' + str(epoch_num) + '_' + '.pt'\n",
        "\n",
        "    checkpoint = torch.load( checkpoint_path )\n",
        "\n",
        "    model.load_state_dict( checkpoint['model_state'] )\n",
        "    optimizer.load_state_dict( checkpoint['optim_state'] )\n",
        "    epoch_scores = checkpoint['epoch_scores']\n",
        "\n",
        "    return model, optimizer, epoch_scores\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEwOMwISpuKo"
      },
      "source": [
        "### Hyperparamter Tuning\n",
        "We use validation data to find the values for our hyperparamters to give us the best model possible.\n",
        "\n",
        "__The hyperparameters we will tune are:__\n",
        "1. Adam Optimizer learning rate\n",
        "2. Number of epochs (In the actual training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KyCu7DXpuKo"
      },
      "source": [
        "#### First, we tune the Adam Optimizer's learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdFw7XVgpuKo"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "def tune_lr(model, train_dataloader, val_dataloader, learning_rate, n_epochs, \n",
        "            model_name, start_epoch=1, checkpoint=None):\n",
        "\n",
        "    loss_function = nn.BCELoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "        loss_function.cuda()\n",
        "\n",
        "    if checkpoint is not None:\n",
        "        model, optimizer, epoch_scores = load_checkpoint(\n",
        "            model=model, optimizer=optimizer, epoch_num=checkpoint['epoch_num'],\n",
        "            learning_rate=checkpoint['lr'], model_name=model_name)\n",
        "\n",
        "    train_acc = val_acc = 0\n",
        "\n",
        "    for epoch_num in range( start_epoch, n_epochs+1 ):\n",
        "\n",
        "        num_correct = num_samples = 0\n",
        "\n",
        "        for train_input, train_label in tqdm(iterable=train_dataloader, desc='Tuning LR'):\n",
        "        \n",
        "            # Forward Propagation\n",
        "            output, label = propagate_forwards(model=model, input=train_input, label=train_label, device=device)\n",
        "            batch_loss = loss_function(output, label)\n",
        "\n",
        "            # Backward Propagation\n",
        "            model.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            predictions = (output > 0.5).long().cpu()\n",
        "            num_correct += (predictions == label.cpu()).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        train_acc = num_correct / num_samples\n",
        "\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            num_correct = num_samples = 0\n",
        "\n",
        "            for val_input, val_label in tqdm(iterable=val_dataloader, desc='Assessing'):\n",
        "\n",
        "                # Forward Propagation\n",
        "                output, label = propagate_forwards(model=model, input=val_input, label=val_label, device=device)\n",
        "                \n",
        "                predictions = (output > 0.5).long().cpu()\n",
        "                num_correct += (predictions == label.cpu()).sum()\n",
        "                num_samples += predictions.size(0)\n",
        "\n",
        "        val_acc = num_correct / num_samples\n",
        "\n",
        "        print(f'\\nTuning epoch complete. | Training accuracy = {train_acc: .3f} | Validation accuracy = {val_acc: .3f}')\n",
        "        print('Saving model checkpoint to disk...')\n",
        "        save_checkpoint( model, optimizer, epoch_num, learning_rate, [], model_name )\n",
        "\n",
        "    print(f'\\x1b[32mAll done :) [LR = {learning_rate}]\\x1b[0m')\n",
        "    \n",
        "    return train_acc, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUYr1OORpuKp"
      },
      "source": [
        "We will tune with an arbitrary number of epochs for now. (Using pre-processed data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv1RPTHfsDvn"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "train, val = Dataset(df_train_proc), Dataset(df_val_proc)\n",
        "# train, = val = Dataset(df_val_proc)\n",
        "N_EPOCHS = 2\n",
        "LR_VALUES = [1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6]\n",
        "BATCH_SIZE = 32\n",
        "lr_scores = []\n",
        "load = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OLekJYqVNCW"
      },
      "outputs": [],
      "source": [
        "lr_scores = torch.load(SAVE_LOAD_DESTINATION + '/lr_scores')\n",
        "load = { # Manually change the values of this dict to load a certain file\n",
        "        'epoch_num':1,\n",
        "        'lr': LR_VALUES[len(lr_scores)]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS1IhIsVpuKp"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
        "\n",
        "idx = len(lr_scores)\n",
        "\n",
        "for idx in range( len(LR_VALUES) ):\n",
        "    LR = LR_VALUES[idx]\n",
        "    neural_network_model_proc = NeuralNetwork(use_dropout=False)\n",
        "    train_acc, val_acc = tune_lr(model=neural_network_model_proc,\n",
        "                                train_dataloader=train_dataloader, \n",
        "                                val_dataloader=val_dataloader, \n",
        "                                learning_rate=LR, n_epochs=N_EPOCHS,\n",
        "                                model_name=\"tune_lr_proc\",\n",
        "                                start_epoch=1,\n",
        "                                checkpoint=load\n",
        "                                )\n",
        "    load = None\n",
        "    \n",
        "    print(f'\\n\\x1b[33m(At LR = {LR}, {N_EPOCHS} epochs) : \\\n",
        "    Training Accuracy = {train_acc: .3f} | \\\n",
        "    Validation Accuracy = {val_acc: .3f}\\n\\x1b[0m\\n')\n",
        "    \n",
        "    lr_scores.append( val_acc )\n",
        "    \n",
        "    torch.save(lr_scores, SAVE_LOAD_DESTINATION + '/lr_scores')\n",
        "\n",
        "plt.plot( LR_VALUES, lr_scores )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SVg-UZ6puKp"
      },
      "source": [
        "(Using raw data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azOQAodrpuKq"
      },
      "outputs": [],
      "source": [
        "# Run yabny\n",
        "\n",
        "train, val = Dataset(df_train_raw), Dataset(df_val_raw)\n",
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "lr_scores = []\n",
        "\n",
        "for LR in LR_VALUES:\n",
        "    neural_network_model_raw = NeuralNetwork(use_dropout=False)\n",
        "    train_acc, val_acc = tune_lr(model=neural_network_model_raw,\n",
        "                                train_dataloader=train_dataloader, \n",
        "                                val_dataloader=val_dataloader, \n",
        "                                learning_rate=LR, n_epochs=N_EPOCHS,\n",
        "                                model_name=\"tune_lr_raw\",\n",
        "                                start_epoch=1,\n",
        "                                checkpoint=load\n",
        "                                )\n",
        "    load = None\n",
        "\n",
        "\n",
        "    print(f'\\n\\x1b[33m(At LR = {LR}, {N_EPOCHS} epochs) : \\\n",
        "    Training Accuracy = {train_acc: .3f} | \\\n",
        "    Validation Accuracy = {val_acc: .3f}\\n\\x1b[0m\\n')\n",
        "\n",
        "    lr_scores.append( val_acc )\n",
        "\n",
        "    torch.save(lr_scores, SAVE_LOAD_DESTINATION + '/lr_scores')\n",
        "\n",
        "plt.plot( LR_VALUES, lr_scores )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHNygJJ7puKq"
      },
      "source": [
        "#### Tuning the number of epochs will take place during the actual training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOFvewXNpuKq"
      },
      "source": [
        "### Training the model using the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wryf3fN_-D8"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, val_data, learning_rate, n_epochs, model_name):\n",
        "\n",
        "    loss_function = nn.BCELoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "        loss_function = loss_function.cuda()\n",
        "\n",
        "    print('\\n------------------------------------')\n",
        "    print('Accelerator Name: ' + str(device))\n",
        "    print('Learning Rate   : ' + str(learning_rate))\n",
        "    print('No. of Epochs   : ' + str(n_epochs))\n",
        "    print('------------------------------------', flush=True)\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    epoch_scores = []\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch_num in range( 1, n_epochs+1 ):\n",
        "\n",
        "        num_correct = num_samples = 0\n",
        "\n",
        "        for train_input, train_label in tqdm(iterable=train_dataloader, desc='Epoch #' + str(epoch_num)):\n",
        "            \n",
        "            # Forward Propagation\n",
        "            output, label = propagate_forwards(model=model, input=train_input, label=train_label, device=device)\n",
        "            batch_loss = loss_function(output, label.float())\n",
        "\n",
        "            # Backward Propagation\n",
        "            model.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            predictions = (output > 0.5).long().cpu()\n",
        "            num_correct += (predictions == label.cpu()).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        train_acc = num_correct / num_samples\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            num_correct = num_samples = 0\n",
        "            \n",
        "            for val_input, val_label in tqdm(iterable=val_dataloader, desc='Assessing'):\n",
        "\n",
        "                # Forward Propagation\n",
        "                output, label = propagate_forwards(model=model, input=val_input, label=val_label, device=device)\n",
        "                \n",
        "                predictions = (output > 0.5).long().cpu()\n",
        "                num_correct += (predictions == label.cpu()).sum()\n",
        "                num_samples += predictions.size(0)\n",
        "\n",
        "        val_acc = num_correct / num_samples\n",
        "\n",
        "        print(f'\\nTraining epoch complete. | Training accuracy = {train_acc: .3f} | Validation accuracy = {val_acc: .3f}')\n",
        "        epoch_scores.append( val_acc )\n",
        "\n",
        "        print('Saving model checkpoint to disk...')\n",
        "        save_checkpoint( model, optimizer, epoch_num, learning_rate, epoch_scores, \"model_proc\" )\n",
        "\n",
        "    print('All done :)')\n",
        "\n",
        "    plt.plot( range( 1, n_epochs+1 ), epoch_scores )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYYDDkSGpuKr"
      },
      "source": [
        "### Evaluating the model using the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyqrlJ6hpuKs"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
        "    \n",
        "    predicted_labels, actual_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for test_input, test_label in tqdm(iterable=test_dataloader, desc='Evaluating'):\n",
        "\n",
        "            output, label = propagate_forwards( model=model, input=test_input, label=test_label, device=device )\n",
        "\n",
        "            predicted_label = torch.round( output.cpu() )\n",
        "\n",
        "            predicted_labels.append( predicted_label )\n",
        "            actual_labels.append( test_label )\n",
        "\n",
        "    \n",
        "    print(classification_report(actual_labels, predicted_labels, target_names=['-', '+']))\n",
        "    ConfusionMatrixDisplay(confusion_matrix(actual_labels, predicted_labels), display_labels=['g', 'h']).plot()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTTXdPWqASKT"
      },
      "source": [
        "## Now we witness the magic :)\n",
        "\n",
        "At this point, we've tuned all our hyperparameters and are ready to test the model for real"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsI5mljVpuKt"
      },
      "source": [
        "### Training the model using pre-processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHcxvuJtAW1D"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 10\n",
        "lr = LEARNING_RATE_PROC\n",
        "print('\\nTraining using pre-processed data', flush=True)\n",
        "train(model=neural_network_model_proc, train_data=df_train_proc, val_data=df_val_proc, learning_rate=lr, n_epochs=N_EPOCHS, model_path='model_proc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rSsqKi6puKt"
      },
      "source": [
        "### Training the model using raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1NcH6yDpuKt"
      },
      "outputs": [],
      "source": [
        "lr = LEARNING_RATE_RAW\n",
        "print('Training using raw data', flush=True)\n",
        "train(model=neural_network_model_raw, train_data=df_train_raw, val_data=df_val_raw, learning_rate=lr, n_epochs=N_EPOCHS, model_path='model_raw')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GomRp-JdpuKu"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpmuk3XPpuKu"
      },
      "source": [
        "#### [OPTIONAL] Leading our pre-trained models from disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM3FzBXypuKu"
      },
      "source": [
        "Load model trained on pre-processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp7HN14BpuKu"
      },
      "outputs": [],
      "source": [
        "### OPTIONAL ###\n",
        "# Loads the model state dictionary from disk.\n",
        "# Run this cell if the trained model is not in main memory (has not been trained in the same runtime)\n",
        "neural_network_model_proc = NeuralNetwork( dropout_probability=0.5 )\n",
        "neural_network_model_proc.load_state_dict( torch.load(\"model_proc.pt\") )\n",
        "neural_network_model_proc.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox84Cxq3puKv"
      },
      "source": [
        "Load model trained on raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-C-atYjpuKv"
      },
      "outputs": [],
      "source": [
        "### OPTIONAL ###\n",
        "# Loads the model state dictionary from disk.\n",
        "# Run this cell if the trained model is not in main memory (has not been trained in the same runtime)\n",
        "neural_network_model_raw = NeuralNetwork( dropout_probability=0.5 )\n",
        "neural_network_model_raw.load_state_dict( torch.load(\"model_raw.pt\") )\n",
        "neural_network_model_raw.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am_v5966puKv"
      },
      "source": [
        "#### Evaluate the model\n",
        "The moment of truth! :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk_LBGXnpuKv"
      },
      "source": [
        "Model evaluation using pre-processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JfcXkmJpuKw"
      },
      "outputs": [],
      "source": [
        "evaluate( model=neural_network_model_proc, test_data=df_test_proc )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEsS3xq0puKw"
      },
      "source": [
        "Model evaluation using raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyK-QITQpuKw"
      },
      "outputs": [],
      "source": [
        "evaluate( model=neural_network_model_raw, test_data=df_test_raw )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "muviY-I2puKZ"
      ],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "92e83001589606ab90afa46080212dcb734f30c24d2f66d52b7ead7b1f127be3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
